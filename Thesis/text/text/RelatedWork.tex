% -------------------------------------------------------------------------------------------------
%      MDSG Latex Framework
%      ============================================================================================
%      File:                  introduction-[UTF8,ISO8859-1].tex
%      Author(s):             Michael Duerr
%      Version:               1
%      Creation Date:         30. Mai 2010
%      Creation Date:         30. Mai 2010
%
%      Notes:                 - Example chapter
% -------------------------------------------------------------------------------------------------
%
\chapter{Related Work}\label{sec:RelatedWork}
- Definition of field of research \\
- Scientific Scope \\
- Which comparable work in research exists? \\
- Separation from other works

\section{Credit Assignment Problem}
\marginpar{intro and comp. problems}
Realistic RL scenarios often involve multiple agents solving problems together, for example robots working in warehouses and factories. Such multiagent environments come with many difficulties. On the one hand in a scenario where agents work independently it is very probable that they get in each other's way in order to score highest or finish a task, preventing the overall goal to be achieved.

\marginpar{coop problems}
In cooperative environments on the other hand, agents share the reward and therefore can not tell who contributed useful actions and who did not. Hence, all agents receive the same reward regardless of their contribution, which aggravates learning. The independence problem is discussed in chapter \ref{market} whereas the cooperation challenge is the focus point of this chapter.

\marginpar{coop and problem}
Sutton and Barto \cite{suba18} define a RL environment as cooperative, when agents execute their actions collectively each time step but receive one overall reward in return. In this case individual learning is difficult or even impossible. Collective actions may contain bad choices that could be rewarded or, in case of a penalty,  good actions that would be punished. Deciding which agent deserves more or less reward, when splitting it up is referred to as the credit assignment problem (CAP) \cite{mi61}.

\marginpar{CAP definition and kinds}
The CAP originated in a one-agent environment that only returned reward once the goal is reached or the terminating condition applied. A popular example of this is a chess game. In 1961, Minsky \cite{mi61} elaborated on this by explaining that a player wins or loses the game, but cannot retrace which decision got him there. Sutton later on decomposed the CAP into subproblems, namely the structural and temporal CAP \cite{su84}. He suggests that the temporal CAP is assigning credit to each chess move, by determining when the position improves or worsen, rewarding or penalizing that certain action. On the contrary, the structural CAP is assigning credit to the internal decision that leads to each particular action.

\marginpar{CAP multi}
% Sutton also refers to a chess game to explain the subproblems. 
Transferring the single-agent CAP into a multiagent environment Agogino and Tumer \cite{agtu04} imply that the problem shifts from being of temporal to structural manner. They explain that while a single agent faces the temporal CAP due to many steps taken within an extended time period, in the multiagent case it becomes a structural CAP because of multiple actions in a single-time-step. Since the actions are executed all at once, the problem is now to evaluate the decision that lies underneath.
% They explain that a single agent faces the temporal CAP when many steps are taken within an extended time period so learning from the returned reward at the end is problematic. Whereas in the multiagent setting it becomes a structural CAP because of multiple actions leading to a shared reward during a single-time-step.

\marginpar{cap solution dr}
Over the years many solutions and theories emerged in order to solve various CAP scenarios. An example for a simple approach is the difference reward (DR) \cite{agtu04},\cite{ngku18}. The idea is to calculate the reward with the joint multiagent actions as always. In every step however, each agent decomposes that reward by calculating the difference between a new reward and the old one. The new reward is generated with the same actions, only modifying the action of the current agent, setting it to a default or waiting value. With this method each agent has the opportunity to learn how they contributed to the resulting state and reward, enabling individual learning. High DR values indicate lucrative actions of the analyzing agent. The opposite case applies for low valued DRs.

\section{Markets}\label{market}
\marginpar{intro mixed motive}
As described earlier, agents that share an environment and act independently can hinder each other from reaching the common and often also the individual goal. Sutton and Barto defined a game to be competitive, when agents receive varying reward signals \cite{suba18}. In this case they are likely to execute actions that are good for themselves even thought they may harm others. The composition is purely competitive, when an agents increase in reward leads to the reward decrease of the others. When the rewards sometimes aligned and sometimes in conflict however, the game is called mixed-motive. Schmid et al. introduced in ``Stochastic Market Games'' \cite{scbe21} some concepts to add incentives when agents act cooperatively in mixed-motive settings in order to improve the rewards for all participants.

The idea is to enable dominant cooperative strategies through global and impartial trading markets.
\marginpar{markets and advantage}
\marginpar{sm}
\marginpar{am}
\marginpar{what is expected?}