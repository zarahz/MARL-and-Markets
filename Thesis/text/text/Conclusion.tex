\chapter{Conclusion}\label{sec:Conclusion}
Each of the three compositions presented in chapter \ref{env} lead to learning problems or game losses. Cooperation may reward misbehavior, namely field resetting, resulting in the CAP of chapter \ref{CAP}. In mixed-motive or fully competitive settings, the overall goal may never be reached, due to greediness or disorder. This research compared the effects of markets not only on mixed-motive settings as suggested by Schmid et al. \cite{scbe21}, but on all three agent compositions.

During the thesis, the coloring environment was presented. It enables agents to color cells by moving around. Stepping on colored cells removes the color and penalizes the agent, coloring cells rewards them. In competitive agent compositions, agents can take over cells and do not reset them. The goal was to color the whole grid in order to get the highest reward. The thesis aims to compare the effect of markets in the three agent compositions and whether SMs or AMs could solve the problems multiagent environments face.

Overall, the CAP does not seem to be solvable through markets applied in the coloring environment. The performance of the DR executions could not be met by any market type. Increasing the grid size and amount of agents also showed that cooperative settings with markets failed to reach the goal completely. This could be an indicator for agents struggling to understand how their actions contributed to the observations and shared rewards. In the mixed-motive and competitive compositions, trainings with markets achieved fully colored grids, even in the difficult setup. This shows an alignment and organization between the agents. Nonetheless, in the most challenging environment, a nine by nine room divided grid, none of the compared settings helped the agents learn how to solve the task.

All in all, one could argue, that the parameters need some adjustments for the different challenges to achieve better results. For instance, reducing the fee in AMs and increasing it for SMs, and generally fine-tuning the hyperparameters of the learning algorithms for each setup. Another point is to conduct all 70 training executions in all challenges, rather than filtering the best settings through the results of an easy training setup. It would also be interesting to compare the results of agents, who always have the full view of the environment with this partially observable implementation.

The coloring environment also leaves room for expansions. More environment shapes could be defined, instead of just a quadratic instance. It would be interesting to see how different reward values for good or bad actions might affect the learning. Another addition that comes to mind would be a supplementary small bonus reward for completely coloring the grid. This should increase the urge of reaching that goal across all compositions.

In regard to the markets, one point to note for future implementations is: the integration of markets into observations. This could enable some sort of micromanagement or at least improve market transparency. Agents could declare in AMs, for example, which action they expect for the next or future time steps. By adding the market actions into the observation, agents in turn could specifically react to the current offer. This also applies for declaring the selling of shares. Agents would know when shares are up for sale and can decide to buy them. 

Another idea for applications of SMs is to use the observation to show the acquired shares an agent has from others, so that the relation to higher rewards is more obvious. Generally, the concept of SMGs, as defined by Schmid et al. \cite{scbe21}, is that agents do not communicate directly, but still find a way to cooperate in mixed-motive compositions. By extending the observation, this concept in theory still holds true.

Lastly, a variable market price could be introduced. This would enable agents to decide how much something is valued. Going back to the AM example, where an agent needs a resource that another agent occupies, depending on the current urgency, the agent in need might be willing to pay a higher price. In SMs, agents could decide to pay a certain share price and, based on that amount, the targeted agent might decide to sell after all. Overall, the subject of markets in RL is a very exciting research topic, due to its wide range of possibilities.