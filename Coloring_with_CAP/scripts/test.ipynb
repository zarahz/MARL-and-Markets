{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\r\n",
    "import argparse\r\n",
    "import time\r\n",
    "import gym\r\n",
    "import torch\r\n",
    "from environment.wrappers import CooperativeMultiagentWrapper\r\n",
    "\r\n",
    "import learning.utils\r\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\r\n",
    "# preparation\r\n",
    "\r\n",
    "# Set seed for all randomness sources\r\n",
    "seed = 0\r\n",
    "learning.utils.seed(seed)\r\n",
    "\r\n",
    "one_agent_env = gym.make(id=\"Empty-Grid-v0\", agents=1, max_steps=15)\r\n",
    "one_agent_env = CooperativeMultiagentWrapper(one_agent_env)\r\n",
    "one_agent_env.seed(seed + 10000)\r\n",
    "\r\n",
    "two_agents_env = gym.make(id=\"Empty-Grid-v0\", agents=2, max_steps=15)\r\n",
    "two_agents_env = CooperativeMultiagentWrapper(two_agents_env)\r\n",
    "two_agents_env.seed(seed + 10000)\r\n",
    "\r\n",
    "three_agents_env = gym.make(id=\"Empty-Grid-v0\", agents=2, max_steps=15)\r\n",
    "three_agents_env = CooperativeMultiagentWrapper(three_agents_env)\r\n",
    "three_agents_env.seed(seed + 10000)\r\n",
    "\r\n",
    "print(\"Environments loaded\\n\")\r\n",
    "\r\n",
    "# Set device\r\n",
    "\r\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
    "print(f\"Device: {device}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load agent\r\n",
    "\r\n",
    "model_dir_one = learning.utils.get_model_dir(\"emptyGrid-one-agent\")\r\n",
    "model_dir_two = learning.utils.get_model_dir(\"emptyGrid-two-agents\")\r\n",
    "model_dir_three = learning.utils.get_model_dir(\"emptyGrid-three-agents\")\r\n",
    "\r\n",
    "one_agent = learning.utils.Agent(0, one_agent_env.observation_space, \r\n",
    "    one_agent_env.action_space, model_dir_one, device=device)\r\n",
    "\r\n",
    "two_agents = []\r\n",
    "for double_agent in range(2):\r\n",
    "    two_agents.append(learning.utils.Agent(double_agent, \r\n",
    "        two_agents_env.observation_space, two_agents_env.action_space, \r\n",
    "        model_dir_two, device=device))\r\n",
    "\r\n",
    "three_agents = []\r\n",
    "for triple_agent in range(2):\r\n",
    "    three_agents.append(learning.utils.Agent(triple_agent, \r\n",
    "        three_agents_env.observation_space, three_agents_env.action_space, \r\n",
    "        model_dir_three, device=device))\r\n",
    "\r\n",
    "print(\"Agents loaded\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather the plotting data\r\n",
    "\r\n",
    "for episode in range(100):\r\n",
    "\r\n",
    "    for step in range(15):\r\n",
    "        \r\n",
    "        joint_actions = []\r\n",
    "        for agent_index, agent in enumerate(agents):\r\n",
    "            action = agent.get_actions(obs, agent_index)\r\n",
    "            joint_actions.append(action)\r\n",
    "\r\n",
    "        obs, reward, done, info = env.step(joint_actions)\r\n",
    "        info[\"resetted_fields\"] = step\r\n",
    "        logs[\"reward_per_episode\"][episode] += reward\r\n",
    "        logs[\"resetted_fields_per_episode\"][episode] += info[\"resetted_fields\"]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ef4c9a1672e459a27e7ff475a1073c81b9bd7c6e0733d9e1b3ac1a72086291d5"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('Coloring_with_CAP-xNNGJax5': pipenv)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}