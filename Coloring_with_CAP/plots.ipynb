{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# imports\r\n",
    "import torch\r\n",
    "\r\n",
    "import learning.ppo.utils\r\n",
    "from matplotlib import pyplot as plt\r\n",
    "import numpy as np\r\n",
    "import seaborn as sns\r\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# preparation\r\n",
    "episodes = 100\r\n",
    "steps = 25\r\n",
    "# Set device\r\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
    "print(f\"Device: {device}\\n\")\r\n",
    "\r\n",
    "# Set seed for all randomness sources\r\n",
    "seed = 0\r\n",
    "\r\n",
    "# init environments\r\n",
    "envs = {\r\n",
    "    \"one-agent\": {\"agent_amount\": 1},\r\n",
    "    \"two-agents\": {\"agent_amount\": 2},\r\n",
    "    \"two-agents-mixed\": {\"agent_amount\": 2, \"mixed\": True},\r\n",
    "    \"two-agents-percentage\": {\r\n",
    "        \"agent_amount\": 2, \r\n",
    "        \"percentage\":True\r\n",
    "    },\r\n",
    "    \"three-agents\": {\"agent_amount\": 3}\r\n",
    "}\r\n",
    "\r\n",
    "# init data logging \r\n",
    "log_data = {}"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Device: cpu\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def load_environment(setting):\r\n",
    "    env_id = \"Empty-Grid-v0\"\r\n",
    "    if \"mixed\" in setting:\r\n",
    "        env = learning.ppo.utils.make_env(\r\n",
    "            env_id, setting[\"agent_amount\"], seed=seed, mixed_motive=setting[\"mixed\"])\r\n",
    "    elif \"percentage\" in setting:\r\n",
    "        env = learning.ppo.utils.make_env(\r\n",
    "            env_id, setting[\"agent_amount\"], seed=seed, percentage_reward=setting[\"percentage\"])\r\n",
    "    else:\r\n",
    "        env = learning.ppo.utils.make_env(\r\n",
    "            env_id, setting[\"agent_amount\"], seed=seed)\r\n",
    "    return env\r\n",
    "\r\n",
    "def load_agents(agent_amount, env, model): \r\n",
    "    model_dir = learning.ppo.utils.get_short_model_dir(model)\r\n",
    "    agents = []\r\n",
    "    if hasattr(env.action_space, 'n'):\r\n",
    "        action_space = env.action_space.n\r\n",
    "    else: # multi dim (market) action \r\n",
    "        action_space = env.action_space.nvec.prod()\r\n",
    "\r\n",
    "    for agent in range(agent_amount):\r\n",
    "        agents.append(learning.ppo.utils.Agent(agent, env.observation_space, \r\n",
    "            action_space, model_dir, device=device))\r\n",
    "    return agents"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "print(env.action_space.nvec)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'Discrete' object has no attribute 'nvec'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11548/1021129917.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnvec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'Discrete' object has no attribute 'nvec'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "for key, env_settings in envs.items():\r\n",
    "    #load environments and their agents\r\n",
    "    env = load_environment(env_settings)\r\n",
    "    agents = load_agents(env_settings[\"agent_amount\"], env, key)\r\n",
    "\r\n",
    "\r\n",
    "    # prepare logging\r\n",
    "    for agent in range(len(agents)):\r\n",
    "        log_data[key+\"-reward-\"+str(agent)] = []\r\n",
    "    log_data[key+\"-reset-fields\"] = []\r\n",
    "    log_data[key+\"-grid-solved\"] = []\r\n",
    "    log_data[key+\"-coloration-percentage\"] = []\r\n",
    "\r\n",
    "    # gather the plotting data\r\n",
    "    for episode in range(episodes):\r\n",
    "        reset_fields = 0\r\n",
    "        obs = env.reset()\r\n",
    "        for step in range(0,steps):\r\n",
    "            joint_actions = []\r\n",
    "            for agent_index, agent in enumerate(agents):\r\n",
    "                action = agent.get_action(obs, agent_index)\r\n",
    "                joint_actions.append(action)\r\n",
    "\r\n",
    "            obs, reward, done, info = env.step(joint_actions)\r\n",
    "            reset_fields += info[\"reset_fields\"]\r\n",
    "            \r\n",
    "            if done or step == steps-1: \r\n",
    "                # save reward here since we work with sparse rewards\r\n",
    "                for agent, agent_reward in enumerate(reward):\r\n",
    "                    log_data[key+\"-reward-\"+str(agent)].append(agent_reward)\r\n",
    "                log_data[key+\"-reset-fields\"].append(reset_fields)\r\n",
    "                log_data[key+\"-coloration-percentage\"].append(info[\"coloration_percentage\"])\r\n",
    "                if 1 in reward:\r\n",
    "                    log_data[key+\"-grid-solved\"].append(1)\r\n",
    "                else:\r\n",
    "                    log_data[key+\"-grid-solved\"].append(0)\r\n",
    "                    \r\n",
    "                break"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "---------> max steps:  25\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, torch.memory_format memory_format, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11548/1142184016.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m#load environments and their agents\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_environment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv_settings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0magents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_agents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv_settings\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"agent_amount\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11548/1254172434.py\u001b[0m in \u001b[0;36mload_agents\u001b[1;34m(agent_amount, env, model)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0magents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0magent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent_amount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         agents.append(learning.ppo.utils.Agent(agent, env.observation_space, \n\u001b[0m\u001b[0;32m     19\u001b[0m             env.action_space, model_dir, device=device))\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0magents\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Zarah\\Documents\\workspace\\MA\\Coloring_with_CAP\\learning\\ppo\\utils\\agent.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, agent_index, obs_space, action_space, model_dir, device, argmax, num_envs)\u001b[0m\n\u001b[0;32m     18\u001b[0m             obs_space)\n\u001b[0;32m     19\u001b[0m         \u001b[1;31m# TODO action-space differ bet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mACModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs_space\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_space\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margmax\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Zarah\\Documents\\workspace\\MA\\Coloring_with_CAP\\learning\\ppo\\model.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, obs_space, action_space)\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTanh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m             \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_space\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m         )\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.virtualenvs\\Coloring_with_CAP-xNNGJax5\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0min_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0min_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mout_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mout_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0min_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfactory_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfactory_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, torch.memory_format memory_format, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "line_styles = [\"-\", \"--\", \"-.\", \":\", \".\", \",\", \"o\", \"v\", \"^\", \"<\", \">\"]\r\n",
    "colors = [\"b\", \"g\", \"r\", \"c\", \"m\", \"y\"]\r\n",
    "\r\n",
    "def plot_data(log_key, title):\r\n",
    "    plt.figure(figsize=(20,10))\r\n",
    "    counter = 0\r\n",
    "    for key, env_settings in envs.items():\r\n",
    "        color = colors[counter] if counter < len(colors) else colors[len(colors)%counter]   \r\n",
    "        if log_key == \"reward\":\r\n",
    "            if \"mixed\" in env_settings and env_settings[\"mixed\"]:\r\n",
    "                # \"mixed\" distinguished between agents, so plot every agent\r\n",
    "                for agent in range(env_settings[\"agent_amount\"]):\r\n",
    "                    line_style = line_styles[agent] if agent < len(line_styles) else line_styles[len(line_styles)%agent]        \r\n",
    "                    plt.plot(log_data[key+\"-\"+log_key+\"-\"+str(agent)], color+line_style, label = key+str(agent))\r\n",
    "            else:\r\n",
    "                # else just plot one agent since values between agents are equal\r\n",
    "                line = color + line_styles[0]\r\n",
    "                plt.plot(log_data[key+\"-\"+log_key+\"-0\"], line, label = key)\r\n",
    "        else:\r\n",
    "            plt.plot(log_data[key+\"-\"+log_key], color, label = key)\r\n",
    "            \r\n",
    "        counter += 1\r\n",
    "\r\n",
    "    plt.ylabel(log_key)\r\n",
    "    plt.xlabel('episode')\r\n",
    "    plt.grid(True)\r\n",
    "    plt.title(title)\r\n",
    "    plt.legend()\r\n",
    "    plt.show()\r\n",
    "\r\n",
    "def plot_barchart(labels, values, ylabel, title):\r\n",
    "    plt.figure(figsize=(20,10))\r\n",
    "    y_pos = np.arange(len(labels))\r\n",
    "    bars = plt.bar(y_pos, values, align='center', alpha=0.5)\r\n",
    "    plt.xticks(y_pos, labels)\r\n",
    "    plt.ylabel(ylabel)\r\n",
    "    plt.title(title)\r\n",
    "    plt.show"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# plot the data\r\n",
    "plot_data(\"reward\", \"Reward per episode with various agents and playmodes (coop vs. mixed) in a 3x3 Grid\")\r\n",
    "plot_data(\"grid-solved\", \"solved grid (100% coloration) with various agents and playmodes (coop vs. mixed) in a 3x3 Grid\")\r\n",
    "plot_data(\"coloration-percentage\", \"solved grid (100% coloration) with various agents and playmodes (coop vs. mixed) in a 3x3 Grid\")\r\n",
    "# plot_data(\"reset_fields\", \"Count of reset grid cells (colored to not colored) per episode with variing agent amount and playmode (coop vs. mixed) in a 3x3 Grid\")\r\n",
    "\r\n",
    "amount_solved_grid_per_setting = {}\r\n",
    "amount_reset_fields_per_setting = {}\r\n",
    "avg_grid_coloration_per_setting = {}\r\n",
    "for setting, value in log_data.items():\r\n",
    "    if \"grid-solved\" in setting:\r\n",
    "        amount_solved_grid_per_setting[setting] = sum(value)\r\n",
    "    if \"reset-fields\" in setting:\r\n",
    "        amount_reset_fields_per_setting[setting] = sum(value)\r\n",
    "    if \"coloration-percentage\" in setting:\r\n",
    "        avg_grid_coloration_per_setting[setting] = sum(value) / len(value)\r\n",
    "\r\n",
    "plot_barchart(amount_solved_grid_per_setting.keys(), amount_solved_grid_per_setting.values(), \"count of solved grid\", \r\n",
    "    \"number of goal achievement (100% coloration) in each setting\")\r\n",
    "\r\n",
    "plot_barchart(amount_reset_fields_per_setting.keys(), amount_reset_fields_per_setting.values(), \"count of reset fields\", \r\n",
    "    \"number of reset fields (colored to not colored cell) in each setting\")\r\n",
    "\r\n",
    "plot_barchart(avg_grid_coloration_per_setting.keys(), avg_grid_coloration_per_setting.values(), \"average grid coloration percentage\", \r\n",
    "    \"average percentage of grid coloration\")\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# create dataframe\r\n",
    "df = pd.DataFrame(log_data)\r\n",
    "add_mixed_cols = False\r\n",
    "\r\n",
    "all_reward_cols = []\r\n",
    "reward_cols = []\r\n",
    "reset_fields_cols = []\r\n",
    "coloration_percentage_cols = []\r\n",
    "grid_solved_cols = []\r\n",
    "for col in df.columns:\r\n",
    "    if \"reward\" in col:\r\n",
    "        all_reward_cols.append(col)\r\n",
    "        if \"mixed\" in col:\r\n",
    "            add_mixed_cols = True\r\n",
    "            reward_cols.append(col)\r\n",
    "        elif \"reward-0\" in col:\r\n",
    "            # in most cases rewards are the same for all agents so only safe one\r\n",
    "            reward_cols.append(col)\r\n",
    "    if \"grid-solved\" in col:\r\n",
    "        grid_solved_cols.append(col)\r\n",
    "    if \"reset-field\" in col:\r\n",
    "        reset_fields_cols.append(col)\r\n",
    "    if \"coloration-percentage\" in col:\r\n",
    "        coloration_percentage_cols.append(col)\r\n",
    "\r\n",
    "if add_mixed_cols:\r\n",
    "    df_mixed = df.filter(regex='mixed-reward')\r\n",
    "    df.insert(0, \"mixed_total\", df_mixed.sum(axis=1))\r\n",
    "    reward_cols.append(\"mixed_total\")\r\n",
    "\r\n",
    "df_reward = df[reward_cols].transpose().stack().reset_index()\r\n",
    "df_reward.rename(columns={0:'reward', \"level_0\": \"setting\", \"level_1\": \"episode\"}, inplace=True)\r\n",
    "\r\n",
    "df_reset_fields = df[reset_fields_cols].transpose().stack().reset_index()\r\n",
    "df_reset_fields.rename(columns={0:'reset_fields', \"level_0\": \"setting\", \"level_1\": \"episode\"}, inplace=True)\r\n",
    "\r\n",
    "df_grid_solved = df[grid_solved_cols].transpose().stack().reset_index()\r\n",
    "df_grid_solved.rename(columns={0:'grid_solved', \"level_0\": \"setting\", \"level_1\": \"episode\"}, inplace=True)\r\n",
    "\r\n",
    "df_coloration_percentage = df[coloration_percentage_cols].transpose().stack().reset_index()\r\n",
    "df_coloration_percentage.rename(columns={0:'pecentage', \"level_0\": \"setting\", \"level_1\": \"episode\"}, inplace=True)\r\n",
    "\r\n",
    "df_combined_reset_and_solved = pd.merge(df_grid_solved, df_reset_fields[\"reset_fields\"], left_index=True, right_index=True, how='outer')\r\n",
    "# df_grid_solved.merge(df_reset_fields[\"reset_fields\"], axis=1) #df.transpose().stack().reset_index()\r\n",
    "print(df_combined_reset_and_solved)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sns.set_theme(style=\"dark\")\r\n",
    "\r\n",
    "# Plot each year's time series in its own facet\r\n",
    "g = sns.relplot(\r\n",
    "    data=df_reward,\r\n",
    "    x=\"episode\", y=\"reward\", col=\"setting\", hue=\"setting\",\r\n",
    "    kind=\"line\", palette=\"crest\", linewidth=4, zorder=5,\r\n",
    "    col_wrap=2, height=5, aspect=3, legend=False,\r\n",
    ")\r\n",
    "\r\n",
    "# Iterate over each subplot to customize further\r\n",
    "for setting, ax in g.axes_dict.items():\r\n",
    "\r\n",
    "    # Add the title as an annotation within the plot\r\n",
    "    ax.text(.8, .85, setting, transform=ax.transAxes, fontweight=\"bold\")\r\n",
    "\r\n",
    "    # Plot every year's time series in the background\r\n",
    "    sns.lineplot(\r\n",
    "        data=df_reward, x=\"episode\", y=\"reward\", units=\"setting\",\r\n",
    "        estimator=None, color=\".7\", linewidth=1, ax=ax,\r\n",
    "    )\r\n",
    "\r\n",
    "# Reduce the frequency of the x axis ticks\r\n",
    "ax.set_xticks(ax.get_xticks()[::2])\r\n",
    "\r\n",
    "# Tweak the supporting aspects of the plot\r\n",
    "g.set_titles(\"\")\r\n",
    "g.set_axis_labels(\"\", \"Reward\")\r\n",
    "g.tight_layout()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sns.catplot(x=\"reset_fields\", y=\"setting\", kind=\"boxen\", data=df_reset_fields)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# seaborn plots\r\n",
    "x = df_reward[\"reward\"]\r\n",
    "\r\n",
    "sns.set_theme(style=\"white\", rc={\"axes.facecolor\": (0, 0, 0, 0)})\r\n",
    "\r\n",
    "# sns.lineplot(data=df[reward_cols].transpose(), ax=ax)\r\n",
    "\r\n",
    "# sns.catplot(data=df[reward_cols], orient=\"h\", kind=\"box\", ax=ax)\r\n",
    "\r\n",
    "# print(data)\r\n",
    "\r\n",
    "# Initialize the FacetGrid object\r\n",
    "pal = sns.cubehelix_palette(10, rot=-.25, light=.7)\r\n",
    "g = sns.FacetGrid(df_reward, row=\"setting\", hue=\"setting\", aspect=15, height=1, palette=pal)\r\n",
    "\r\n",
    "# Draw the densities in a few steps\r\n",
    "g.map(sns.kdeplot, \"reward\", \r\n",
    "      bw_adjust=.5, clip_on=False,\r\n",
    "      fill=True, alpha=1, linewidth=1.5)\r\n",
    "g.map(sns.kdeplot, \"reward\", clip_on=False, color=\"w\", lw=2, bw_adjust=.5)\r\n",
    "g.map(plt.axhline, y=0, lw=2, clip_on=False)\r\n",
    "\r\n",
    "# Define and use a simple function to label the plot in axes coordinates\r\n",
    "def label(df_reward, color, label):\r\n",
    "    ax = plt.gca()\r\n",
    "    ax.text(0, .2, label, fontweight=\"bold\", color=color,\r\n",
    "            ha=\"left\", va=\"center\", transform=ax.transAxes)\r\n",
    "\r\n",
    "g.map(label, \"reward\")\r\n",
    "\r\n",
    "# Set the subplots to overlap\r\n",
    "g.fig.subplots_adjust(hspace=-.25)\r\n",
    "\r\n",
    "# Remove axes details that don't play well with overlap\r\n",
    "g.set_titles(\"\")\r\n",
    "g.set(yticks=[])\r\n",
    "g.despine(bottom=True, left=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sns.set_theme(style=\"darkgrid\")\r\n",
    "sns.displot(\r\n",
    "    df_combined_reset_and_solved, x=\"reset_fields\", col=\"setting\", row=\"grid_solved\",\r\n",
    "    binwidth=1, height=5, facet_kws=dict(margin_titles=True),\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ef4c9a1672e459a27e7ff475a1073c81b9bd7c6e0733d9e1b3ac1a72086291d5"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('Coloring_with_CAP-xNNGJax5': pipenv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}